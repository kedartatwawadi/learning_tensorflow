{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but TensorFlow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.50094748] [ 0.08768004]\n",
      "20 [ 0.1893522] [ 0.24759975]\n",
      "40 [ 0.12041937] [ 0.28802514]\n",
      "60 [ 0.10466637] [ 0.29726344]\n",
      "80 [ 0.1010664] [ 0.29937464]\n",
      "100 [ 0.1002437] [ 0.29985711]\n",
      "120 [ 0.10005569] [ 0.29996735]\n",
      "140 [ 0.10001273] [ 0.29999256]\n",
      "160 [ 0.10000292] [ 0.29999831]\n",
      "180 [ 0.10000066] [ 0.29999962]\n",
      "200 [ 0.10000014] [ 0.29999992]\n"
     ]
    }
   ],
   "source": [
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x7f6dd3022400>\n",
      "<class 'tensorflow.python.framework.ops.Operation'>\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(100,), dtype=float32)\n",
      "<tensorflow.python.ops.variables.Variable object at 0x7f6dd3022588>\n"
     ]
    }
   ],
   "source": [
    "# Intensor flow, there are some tensors and some operations.\n",
    "# I am not sure if Variable is an operation, or if in general any operation acts on Variables.\n",
    "# I think so, as I think operations change the variable.\n",
    "# For example, the the init operation will initialize all the variables.\n",
    "# Now, we have some other operations for example:\n",
    "# y = W*x + b. I think here, we are combining variables to form another variable y\n",
    "# Okay, so y is actually a Tensor, and not a tf.Variable. This is interesting. I was wrong in this case.\n",
    "\n",
    "# So, y is a tensor. What is loss?\n",
    "# Interesting, printing tensors in python gives some information regarding what they are?! add, Mean?\n",
    "# So, train IS an operation which modifies all the variables involved in the graph using gradient descent. \n",
    "# and 0.5 seems like the learning rate? need to check this.\n",
    "# AS it is a \"graph\" it can actually backprop pretty easily. Any linear operation can flow through pretty easily through \n",
    "# graph.\n",
    "\n",
    "print(optimizer)\n",
    "print(type(train))\n",
    "print(loss)\n",
    "print(y)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
